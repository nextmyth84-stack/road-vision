# =====================================
# app.py â€” ë„ë¡œì£¼í–‰ ê·¼ë¬´ìë™ë°°ì • v7.18.2 (êµì •Â·ì½”ìŠ¤ì™„ì „ë³¸)
# =====================================
import streamlit as st
from openai import OpenAI
import base64, re, json, os, difflib

st.set_page_config(page_title="ë„ë¡œì£¼í–‰ ê·¼ë¬´ìë™ë°°ì •", layout="wide")
st.markdown("<h3 style='text-align:center; font-size:22px;'>ğŸš— ë„ë¡œì£¼í–‰ ê·¼ë¬´ìë™ë°°ì •</h3>", unsafe_allow_html=True)

# =====================================
# OpenAI ì´ˆê¸°í™”
# =====================================
try:
    client = OpenAI(api_key=st.secrets["general"]["OPENAI_API_KEY"])
except Exception:
    st.error("âš ï¸ OPENAI_API_KEY ì„¤ì • í•„ìš”")
    st.stop()
MODEL_NAME = "gpt-4o"

# =====================================
# JSON ìœ í‹¸
# =====================================
def load_json(path, default=None):
    if os.path.exists(path):
        try:
            with open(path, "r", encoding="utf-8") as f:
                return json.load(f)
        except:
            return default if default is not None else []
    return default if default is not None else []

def save_json(path, data):
    try:
        with open(path, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
    except Exception as e:
        st.error(f"{path} ì €ì¥ ì‹¤íŒ¨: {e}")

# =====================================
# êµì • ì•Œê³ ë¦¬ì¦˜ (correct_name_v3)
#  - ìëª¨ ìœ ì‚¬ë„ + difflib í‰ê· 
#  - OCR í˜¼ë™ì(ã…â†”ã…‚, ã…¡â†”ã…œ ë“±) í—ˆìš©
#  - ìƒëŒ€ì ìˆ˜ 0.08 ë¯¸ë§Œì´ë©´ êµì • ë³´ë¥˜
# =====================================
CHO_LIST = list("ã„±ã„²ã„´ã„·ã„¸ã„¹ã…ã…‚ã…ƒã……ã…†ã…‡ã…ˆã…‰ã…Šã…‹ã…Œã…ã…")
JUNG_LIST = list("ã…ã…ã…‘ã…’ã…“ã…”ã…•ã…–ã…—ã…˜ã…™ã…šã…›ã…œã…ã…ã…Ÿã… ã…¡ã…¢ã…£")
JONG_LIST = [""] + list("ã„±ã„²ã„³ã„´ã„µã„¶ã„·ã„¹ã„ºã„»ã„¼ã„½ã„¾ã„¿ã…€ã…ã…‚ã…„ã……ã…†ã…‡ã…ˆã…Šã…‹ã…Œã…ã…")

CONFUSABLES = {
    'ã…': ['ã…‚'], 'ã…‚': ['ã…'],
    'ã„´': ['ã„¹'], 'ã„¹': ['ã„´'],
    'ã„±': ['ã…‹'], 'ã…‹': ['ã„±'],
    'ã……': ['ã…ˆ','ã…Š'], 'ã…ˆ': ['ã……','ã…Š'], 'ã…Š': ['ã…ˆ','ã……'],
    'ã…': ['ã…”'], 'ã…”': ['ã…'],
    'ã…¡': ['ã…œ'], 'ã…œ': ['ã…¡'],
}

def split_hangul(c):
    code = ord(c) - 0xAC00
    cho = code // 588
    jung = (code - cho * 588) // 28
    jong = code % 28
    return cho, jung, jong

def similar_jamo(a, b):
    return a == b or (a in CONFUSABLES and b in CONFUSABLES[a])

def hangul_similarity(a, b):
    if not a or not b: return 0
    score, total = 0, max(len(a), len(b))
    for i in range(min(len(a), len(b))):
        ca, cb = a[i], b[i]
        if not ('ê°€' <= ca <= 'í£' and 'ê°€' <= cb <= 'í£'):
            score += 1 if ca == cb else 0
            continue
        cho_a, jung_a, jong_a = split_hangul(ca)
        cho_b, jung_b, jong_b = split_hangul(cb)
        s = 0
        if similar_jamo(CHO_LIST[cho_a], CHO_LIST[cho_b]): s += 0.4
        if similar_jamo(JUNG_LIST[jung_a], JUNG_LIST[jung_b]): s += 0.4
        if similar_jamo(JONG_LIST[jong_a], JONG_LIST[jong_b]): s += 0.2
        score += s
    return score / total

def normalized_name(s):
    return re.sub(r"[^ê°€-í£]", "", s or "")

def correct_name_v3(name, valid_names, cutoff=0.6):
    if not name or not valid_names: return name
    n = normalized_name(name)
    best = (None, 0); scores = []
    for valid in valid_names:
        v = normalized_name(valid)
        jamo_score = hangul_similarity(n, v)
        seq_score = difflib.SequenceMatcher(None, n, v).ratio()
        score = (jamo_score + seq_score) / 2
        scores.append((valid, score))
        if score > best[1]: best = (valid, score)
    scores.sort(key=lambda x: x[1], reverse=True)
    if len(scores) > 1 and (scores[0][1] - scores[1][1]) < 0.08:
        return name  # ë¹„ìŠ·í•œ í›„ë³´ ì—¬ëŸ¿ â†’ êµì • ë³´ë¥˜
    return best[0] if best[1] >= cutoff else name

# normalize_name ë³„ì¹­ (ê¸°ì¡´ ì½”ë“œ í˜¸í™˜)
def normalize_name(s): 
    return normalized_name(s)

# =====================================
# ì½”ìŠ¤ ì¶”ì¶œ + ë°°ì •ìš© ì´ë¦„ ì •ë¦¬
# =====================================
def extract_course_from_token(token: str):
    """ê´„í˜¸ ë‚´ 'A'/'B' + 'í•©/ë¶ˆ'ë§Œ ì¶”ì¶œ (íŠ¹ìˆ˜ë¬¸ì ì œê±° í›„ íŒë³„)"""
    m = re.search(r"\((.*?)\)", token)
    if not m: return None
    raw = re.sub(r"[^A-Za-zê°€-í£]", "", m.group(1)).upper()
    course = "A" if "A" in raw else ("B" if "B" in raw else None)
    result = "í•©ê²©" if "í•©" in raw else ("ë¶ˆí•©ê²©" if "ë¶ˆ" in raw else None)
    if course and result:
        return course, result
    return None

def split_and_clean_name_list(raw_names: list, valid_names: list):
    """
    1) correct_name_v3ë¡œ êµì •
    2) ê´„í˜¸ì—ì„œ ì½”ìŠ¤ ì¶”ì¶œ â†’ ê¸°ë¡
    3) ë°°ì •ìš© ì´ë¦„ì€ ê´„í˜¸ ì œê±°í•´ì„œ ë°˜í™˜
    """
    course_records, cleaned = [], []
    for token in raw_names:
        fixed = correct_name_v3(token, valid_names)
        info = extract_course_from_token(fixed)
        pure = re.sub(r"\(.*?\)", "", fixed).strip()
        if info:
            c, r = info
            course_records.append({"name": pure, "course": f"{c}ì½”ìŠ¤", "result": r})
        cleaned.append(pure)
    return cleaned, course_records

# =====================================
# íŒŒì¼ ê²½ë¡œ / ê¸°ë³¸ ë°ì´í„°
# =====================================
EMP_FILE   = "employee_list.json"
KEY_FILE   = "data_key.json"
GY_FILE    = "data_gyoyang.json"
SUD_FILE   = "data_sudong.json"
VEH1_FILE  = "veh1.json"
VEH2_FILE  = "veh2.json"
PREV_FILE  = "ì „ì¼ê·¼ë¬´.json"
COURSE_FILE= "course_result.json"

default_key = ["ê¶Œí•œì†”","ê¹€ë‚¨ê· ","ê¹€ë©´ì •","ê¹€ì„±ì—°","ê¹€ì§€ì€","ì•ˆìœ ë¯¸","ìœ¤ì—¬í—Œ","ìœ¤ì›ì‹¤","ì´ë‚˜ë˜","ì´í˜¸ì„","ì¡°ìœ¤ì˜","ì¡°ì •ë˜"]
default_gy  = ["ê¶Œí•œì†”","ê¹€ë‚¨ê· ","ê¹€ë©´ì •","ê¹€ë³‘ìš±","ê¹€ì„±ì—°","ê¹€ì£¼í˜„","ê¹€ì§€ì€","ì•ˆìœ ë¯¸","ì´í˜¸ì„","ì¡°ì •ë˜"]
default_sd  = ["ê¶Œí•œì†”","ê¹€ë‚¨ê· ","ê¹€ì„±ì—°","ê¹€ì£¼í˜„","ì´í˜¸ì„","ì¡°ì •ë˜"]
default_veh1= ["2í˜¸ ì¡°ì •ë˜","5í˜¸ ê¶Œí•œì†”","7í˜¸ ê¹€ë‚¨ê· ","8í˜¸ ì´í˜¸ì„","9í˜¸ ê¹€ì£¼í˜„","10í˜¸ ê¹€ì„±ì—°"]
default_veh2= ["4í˜¸ ê¹€ë‚¨ê· ","5í˜¸ ê¹€ë³‘ìš±","6í˜¸ ê¹€ì§€ì€","12í˜¸ ì•ˆìœ ë¯¸","14í˜¸ ê¹€ë©´ì •","15í˜¸ ì´í˜¸ì„","17í˜¸ ê¹€ì„±ì—°","18í˜¸ ê¶Œí•œì†”","19í˜¸ ê¹€ì£¼í˜„","22í˜¸ ì¡°ì •ë˜"]
default_emp = sorted(list({*default_key, *default_gy, *default_sd, "ê¹€ë³‘ìš±","ê¹€ì£¼í˜„"}))

# ë¡œë“œ
employee_list = load_json(EMP_FILE, default_emp)
key_order   = load_json(KEY_FILE,  default_key)
gyoyang_ord = load_json(GY_FILE,   default_gy)
sudong_ord  = load_json(SUD_FILE,  default_sd)
veh1_lines  = load_json(VEH1_FILE, default_veh1)
veh2_lines  = load_json(VEH2_FILE, default_veh2)

# -------------------------------------
# ì‚¬ì´ë“œë°”: ì „ì²´ ê·¼ë¬´ì (ìˆ¨ê¹€, ìˆ˜ì •/ì €ì¥)
# -------------------------------------
st.sidebar.header("ê·¼ë¬´ ë°ì´í„° ê´€ë¦¬")
with st.sidebar.expander("ğŸ‘¥ ì „ì²´ ê·¼ë¬´ìëª…ë‹¨", expanded=False):
    emp_edit = st.text_area("ì „ì²´ ê·¼ë¬´ì", "\n".join(employee_list), height=180)
    if st.button("ğŸ’¾ ê·¼ë¬´ìëª…ë‹¨ ì €ì¥"):
        new_list = [x.strip() for x in emp_edit.splitlines() if x.strip()]
        save_json(EMP_FILE, new_list)
        employee_list = new_list
        st.sidebar.success("ì „ì²´ ê·¼ë¬´ì ì €ì¥ ì™„ë£Œ")

# -------------------------------------
# ì‚¬ì´ë“œë°”: ìˆœë²ˆ/ì°¨ëŸ‰í‘œ (ìˆ¨ê¹€, ìˆ˜ì •/ì €ì¥)
# -------------------------------------
with st.sidebar.expander("ğŸ”‘ ì—´ì‡  ìˆœë²ˆ", expanded=False):
    key_edit = st.text_area("ì—´ì‡  ìˆœë²ˆ", "\n".join(key_order), height=140)
    if st.button("ğŸ’¾ ì—´ì‡  ì €ì¥"):
        save_json(KEY_FILE, [x.strip() for x in key_edit.splitlines() if x.strip()])
        key_order = load_json(KEY_FILE, default_key)
        st.sidebar.success("ì—´ì‡  ìˆœë²ˆ ì €ì¥ ì™„ë£Œ")

with st.sidebar.expander("ğŸ“˜ êµì–‘ ìˆœë²ˆ", expanded=False):
    gy_edit = st.text_area("êµì–‘ ìˆœë²ˆ", "\n".join(gyoyang_ord), height=140)
    if st.button("ğŸ’¾ êµì–‘ ì €ì¥"):
        save_json(GY_FILE, [x.strip() for x in gy_edit.splitlines() if x.strip()])
        gyoyang_ord = load_json(GY_FILE, default_gy)
        st.sidebar.success("êµì–‘ ìˆœë²ˆ ì €ì¥ ì™„ë£Œ")

with st.sidebar.expander("ğŸ§° 1ì¢… ìˆ˜ë™ ìˆœë²ˆ", expanded=False):
    sd_edit = st.text_area("1ì¢… ìˆ˜ë™ ìˆœë²ˆ", "\n".join(sudong_ord), height=140)
    if st.button("ğŸ’¾ 1ì¢… ì €ì¥"):
        save_json(SUD_FILE, [x.strip() for x in sd_edit.splitlines() if x.strip()])
        sudong_ord = load_json(SUD_FILE, default_sd)
        st.sidebar.success("1ì¢… ìˆœë²ˆ ì €ì¥ ì™„ë£Œ")

with st.sidebar.expander("ğŸš— ì°¨ëŸ‰í‘œ (1ì¢…/2ì¢…)", expanded=False):
    v1_edit = st.text_area("1ì¢… ìˆ˜ë™ ì°¨ëŸ‰í‘œ", "\n".join(veh1_lines), height=110)
    v2_edit = st.text_area("2ì¢… ìë™ ì°¨ëŸ‰í‘œ", "\n".join(veh2_lines), height=150)
    if st.button("ğŸ’¾ ì°¨ëŸ‰í‘œ ì €ì¥"):
        save_json(VEH1_FILE, [x.strip() for x in v1_edit.splitlines() if x.strip()])
        save_json(VEH2_FILE, [x.strip() for x in v2_edit.splitlines() if x.strip()])
        veh1_lines = load_json(VEH1_FILE, default_veh1)
        veh2_lines = load_json(VEH2_FILE, default_veh2)
        st.sidebar.success("ì°¨ëŸ‰í‘œ ì €ì¥ ì™„ë£Œ")

sudong_count = st.sidebar.radio("1ì¢… ìˆ˜ë™ ì¸ì›ìˆ˜", [1, 2], index=0)
repair_cars = [x.strip() for x in st.sidebar.text_input("ì •ë¹„ ì°¨ëŸ‰ (ì‰¼í‘œë¡œ êµ¬ë¶„)", value="").split(",") if x.strip()]

# ì „ì¼ê°’
prev_data = load_json(PREV_FILE, {"ì—´ì‡ ":"","êµì–‘_5êµì‹œ":"","1ì¢…ìˆ˜ë™":""})
prev_key  = prev_data.get("ì—´ì‡ ","")
prev_gy5  = prev_data.get("êµì–‘_5êµì‹œ","")
prev_sd   = prev_data.get("1ì¢…ìˆ˜ë™","")
st.sidebar.info(f"ì „ì¼ ê¸°ì¤€ â†’ ì—´ì‡ :{prev_key or '-'}, êµì–‘5:{prev_gy5 or '-'}, 1ì¢…:{prev_sd or '-'}")

# =====================================
# GPT OCR: ê·¼ë¬´ì/ì œì™¸ì/ì¡°í‡´/ì§€ê° (ì¡°í‡´/ì§€ê°ì€ í•„ìš”ì‹œ í™•ì¥)
# =====================================
def gpt_extract(img_bytes, want_early=False, want_late=False, want_excluded=False):
    b64 = base64.b64encode(img_bytes).decode()
    user = (
        "ì´ ì´ë¯¸ì§€ëŠ” ìš´ì „ë©´í—ˆì‹œí—˜ ê·¼ë¬´í‘œì…ë‹ˆë‹¤.\n"
        "1) ë„ë¡œì£¼í–‰ ê·¼ë¬´ì ì´ë¦„ë§Œ names ë°°ì—´ë¡œ ì¶”ì¶œí•˜ì„¸ìš”.\n"
        "2) ê´„í˜¸(A-í•©, B-ë¶ˆ ë“±)ê°€ ìˆìœ¼ë©´ ê·¸ëŒ€ë¡œ ë‘” ì›ë¬¸ì„ namesì— ë„£ìœ¼ì„¸ìš”.\n"
        "3) íœ´ê°€/êµìœ¡/ì¶œì¥/ê³µê°€/ì—°ê°€/ì—°ì°¨/ëŒë´„ ë“± í‘œê¸°ëœ ì´ë¦„ì€ excludedì— ë„£ìœ¼ì„¸ìš”.\n"
        + ("4) 'ì¡°í‡´:' ìˆìœ¼ë©´ early_leave: [{name, time(ìˆ«ì)}].\n" if want_early else "")
        + ("5) '10ì‹œ ì¶œê·¼'/'ì™¸ì¶œ:' ìˆìœ¼ë©´ late_start: [{name, time(ìˆ«ì)}].\n" if want_late else "")
        + 'JSONë§Œ ë°˜í™˜. ì˜ˆ: {"names":["ê¹€ì„±ì—°(A-í•©)","ì´í˜¸ì„(B-ë¶ˆ)"], "excluded":["ìœ¤ì›ì‹¤"]}'
    )
    try:
        res = client.chat.completions.create(
            model=MODEL_NAME,
            messages=[
                {"role":"system","content":"í‘œì—ì„œ ì´ë¦„/ì œì™¸ì ì •ë³´ë¥¼ JSONìœ¼ë¡œ ì¶”ì¶œ"},
                {"role":"user","content":[
                    {"type":"text","text":user},
                    {"type":"image_url","image_url":{"url":f"data:image/jpeg;base64,{b64}"}}
                ]}
            ]
        )
        raw = res.choices[0].message.content
        js = json.loads(re.search(r"\{.*\}", raw, re.S).group(0))
        return (
            js.get("names", []),
            js.get("excluded", []),
            js.get("early_leave", []) if want_early else [],
            js.get("late_start", []) if want_late else [],
        )
    except Exception as e:
        st.error(f"OCR ì‹¤íŒ¨: {e}")
        return [], [], [], []

# =====================================
# 1) ì´ë¯¸ì§€ ì—…ë¡œë“œ & OCR
# =====================================
st.markdown("<h4 style='margin-top:6px;'>1ï¸âƒ£ ê·¼ë¬´í‘œ ì´ë¯¸ì§€ ì—…ë¡œë“œ & OCR</h4>", unsafe_allow_html=True)
c1, c2 = st.columns(2)
with c1: m_file = st.file_uploader("ğŸ“¸ ì˜¤ì „ ê·¼ë¬´í‘œ", type=["png","jpg","jpeg"])
with c2: a_file = st.file_uploader("ğŸ“¸ ì˜¤í›„ ê·¼ë¬´í‘œ", type=["png","jpg","jpeg"])

b1, b2 = st.columns(2)
with b1:
    if st.button("ğŸ§  ì˜¤ì „ GPT ì¸ì‹"):
        if not m_file:
            st.warning("ì˜¤ì „ ì´ë¯¸ì§€ë¥¼ ì—…ë¡œë“œí•˜ì„¸ìš”.")
        else:
            with st.spinner("ì˜¤ì „ GPT ë¶„ì„ ì¤‘..."):
                m_names, excluded_auto, _, late = gpt_extract(m_file.read(), want_late=True, want_excluded=True)
                st.session_state.m_names_raw = m_names
                st.session_state.excluded_auto = excluded_auto
                st.session_state.late_start = late
                st.success(f"ì˜¤ì „ ì¸ì‹ â†’ ê·¼ë¬´ì {len(m_names)}ëª…, ì œì™¸ì {len(excluded_auto)}ëª…")

with b2:
    if st.button("ğŸ§  ì˜¤í›„ GPT ì¸ì‹"):
        if not a_file:
            st.warning("ì˜¤í›„ ì´ë¯¸ì§€ë¥¼ ì—…ë¡œë“œí•˜ì„¸ìš”.")
        else:
            with st.spinner("ì˜¤í›„ GPT ë¶„ì„ ì¤‘..."):
                a_names, _, early, _ = gpt_extract(a_file.read(), want_early=True)
                st.session_state.a_names_raw = a_names
                st.session_state.early_leave = early
                st.success(f"ì˜¤í›„ ì¸ì‹ â†’ ê·¼ë¬´ì {len(a_names)}ëª…, ì¡°í‡´ {len(early)}ëª…")

# =====================================
# 2) ì¸ì‹ ê²°ê³¼ í™•ì¸/ìˆ˜ì • (ìŠ¤í¬ë¡¤ ì ìš©)
# =====================================
st.markdown("<h6 style='font-size:15px; font-weight:bold; margin-top:10px;'>ê·¼ë¬´ì œì™¸ì</h6>", unsafe_allow_html=True)
excluded_raw = "\n".join(st.session_state.get("excluded_auto", []))
excluded_text = st.text_area("ìë™ì¶”ì¶œ í›„ ìˆ˜ì • ê°€ëŠ¥", excluded_raw, height=110, label_visibility="collapsed")

st.markdown("<h5 style='margin-top:6px;'>ğŸŒ… ì˜¤ì „ ê·¼ë¬´ì (ìˆ˜ì • ê°€ëŠ¥)</h5>", unsafe_allow_html=True)
morning_raw = "\n".join(st.session_state.get("m_names_raw", []))
morning_text = st.text_area("ì˜¤ì „ ê·¼ë¬´ì", morning_raw, height=220)

st.markdown("<h5 style='margin-top:6px;'>ğŸŒ‡ ì˜¤í›„ ê·¼ë¬´ì (ìˆ˜ì • ê°€ëŠ¥)</h5>", unsafe_allow_html=True)
afternoon_raw = "\n".join(st.session_state.get("a_names_raw", []))
afternoon_text = st.text_area("ì˜¤í›„ ê·¼ë¬´ì", afternoon_raw, height=220)

# ë¦¬ìŠ¤íŠ¸ ë³€í™˜
m_list = [x.strip() for x in morning_text.splitlines() if x.strip()]
a_list = [x.strip() for x in afternoon_text.splitlines() if x.strip()]
excluded = {x.strip() for x in excluded_text.splitlines() if x.strip()}

# =====================================
# ë³´ì¡° í•¨ìˆ˜
# =====================================
def parse_vehicle_map(lines):
    m = {}
    for line in lines:
        parts = line.strip().split()
        if len(parts) >= 2:
            car = parts[0]
            name = " ".join(parts[1:])
            m[name] = car
    return m

def get_vehicle(name, veh_map):
    nkey = normalize_name(name)
    for k, v in veh_map.items():
        if normalize_name(k) == nkey:
            return v
    return ""

def mark_car(car, repair_list):
    return f"{car}{' (ì •ë¹„)' if car in repair_list else ''}" if car else ""

def pick_next_from_cycle(cycle, last, allowed_norms: set):
    if not cycle: return None
    cyc_norm = [normalize_name(x) for x in cycle]
    last_norm = normalize_name(last)
    start = (cyc_norm.index(last_norm) + 1) % len(cycle) if last_norm in cyc_norm else 0
    for i in range(len(cycle)*2):
        cand = cycle[(start+i) % len(cycle)]
        if normalize_name(cand) in allowed_norms:
            return cand
    return None

def clipboard_copy_button(label, text):
    btn_id = f"btn_{abs(hash(label+text))}"
    st.markdown(
        f"""
        <button id="{btn_id}" style="background:#2563eb;color:white;border:none;
        padding:6px 12px;border-radius:8px;cursor:pointer;margin-top:6px;">{label}</button>
        <script>
        const b=document.getElementById("{btn_id}");
        b.onclick=()=>{{navigator.clipboard.writeText(`{text}`);b.innerText="âœ… ë³µì‚¬ë¨!";setTimeout(()=>b.innerText="{label}",1500);}};
        </script>
        """, unsafe_allow_html=True
    )

# =====================================
# 3ï¸âƒ£ ì˜¤ì „ ë°°ì •
# =====================================
st.markdown("---")
st.markdown("## 3ï¸âƒ£ ì˜¤ì „ ê·¼ë¬´ ë°°ì •")

if st.button("ğŸ“‹ ì˜¤ì „ ë°°ì • ìƒì„±"):
    try:
        veh1_map = parse_vehicle_map(veh1_lines)
        veh2_map = parse_vehicle_map(veh2_lines)

        # êµì • + ê´„í˜¸ ë¶„ë¦¬ â†’ ë°°ì •ìš© ì´ë¦„ / ì½”ìŠ¤ê¸°ë¡
        cleaned_m, course_records = split_and_clean_name_list(m_list, employee_list)
        save_json(COURSE_FILE, course_records)

        excl_norms = {normalize_name(x) for x in excluded}
        m_norms = {normalize_name(x) for x in cleaned_m} - excl_norms

        # ğŸ”‘ ì—´ì‡ 
        key_filtered = [x for x in key_order if normalize_name(x) not in excl_norms]
        if key_filtered:
            knorms = [normalize_name(x) for x in key_filtered]
            pnorm = normalize_name(prev_key)
            today_key = key_filtered[(knorms.index(pnorm)+1)%len(key_filtered)] if pnorm in knorms else key_filtered[0]
        else:
            today_key = ""

        # ğŸ§‘â€ğŸ« ì˜¤ì „ êµì–‘ 1Â·2êµì‹œ
        gy1 = pick_next_from_cycle(gyoyang_ord, prev_gy5, m_norms)
        gy1_norm = normalize_name(gy1) if gy1 else None
        gy2 = pick_next_from_cycle(gyoyang_ord, gy1 or prev_gy5, m_norms - ({gy1_norm} if gy1_norm else set()))

        # ğŸ”§ 1ì¢… ìˆ˜ë™
        sud_m, last_pick = [], prev_sd
        for _ in range(sudong_count):
            cand = pick_next_from_cycle(sudong_ord, last_pick, m_norms - {normalize_name(x) for x in sud_m})
            if not cand: break
            sud_m.append(cand)
            last_pick = cand

        # ğŸš— 2ì¢… ìë™ = ì „ì²´ - 1ì¢…
        sud_norms_m = {normalize_name(x) for x in sud_m}
        auto_m = [nm for nm in cleaned_m if normalize_name(nm) in (m_norms - sud_norms_m)]

        # ìƒíƒœ ì €ì¥(ì˜¤ì „ ëŒ€ë¹„/ì°¨ëŸ‰ ë¹„êµìš©)
        st.session_state["today_key"] = today_key
        st.session_state["am_gy_base_for_pm"] = gy2 or gy1 or prev_gy5
        st.session_state["am_sud_base_for_pm"] = sud_m[-1] if sud_m else prev_sd
        st.session_state["am_driver_names"] = sud_m + auto_m
        st.session_state["am_cars_1"] = [get_vehicle(x, veh1_map) for x in sud_m if get_vehicle(x, veh1_map)]
        st.session_state["am_cars_2"] = [get_vehicle(x, veh2_map) for x in auto_m if get_vehicle(x, veh2_map)]

        # === ì¶œë ¥
        lines = []
        if today_key: lines.append(f"ì—´ì‡ : {today_key}")
        if gy1: lines.append(f"1êµì‹œ(êµì–‘): {gy1}")
        if gy2: lines.append(f"2êµì‹œ(êµì–‘): {gy2}")

        if sud_m:
            for nm in sud_m:
                lines.append(f"1ì¢…ìˆ˜ë™: {nm} {mark_car(get_vehicle(nm, veh1_map), repair_cars)}")
            if sudong_count == 2 and len(sud_m) < 2:
                lines.append("â€» ìˆ˜ë™ ê°€ëŠ¥ ì¸ì›ì´ 1ëª…ì…ë‹ˆë‹¤.")
        else:
            lines.append("1ì¢…ìˆ˜ë™: (ë°°ì •ì ì—†ìŒ)")

        if auto_m:
            lines.append("2ì¢… ìë™:")
            for nm in auto_m:
                lines.append(f" â€¢ {nm} {mark_car(get_vehicle(nm, veh2_map), repair_cars)}")
        else:
            lines.append("2ì¢… ìë™: (ë°°ì •ì ì—†ìŒ)")

        # ğŸ§­ ì½”ìŠ¤ì ê²€(ì˜¤ì „ë§Œ)
        if course_records:
            lines.append("")
            lines.append("ğŸ§­ ì½”ìŠ¤ì ê²€ ê²°ê³¼:")
            for c in ["A","B"]:
                passed = [r["name"] for r in course_records if r["course"]==f"{c}ì½”ìŠ¤" and r["result"]=="í•©ê²©"]
                failed = [r["name"] for r in course_records if r["course"]==f"{c}ì½”ìŠ¤" and r["result"]=="ë¶ˆí•©ê²©"]
                if passed: lines.append(f" â€¢ {c}ì½”ìŠ¤ í•©ê²©: {', '.join(passed)}")
                if failed: lines.append(f" â€¢ {c}ì½”ìŠ¤ ë¶ˆí•©ê²©: {', '.join(failed)}")

        am_text = "\n".join(lines)
        st.markdown("### ğŸ“‹ ì˜¤ì „ ê²°ê³¼")
        st.code(am_text, language="text")
        clipboard_copy_button("ğŸ“‹ ê²°ê³¼ ë³µì‚¬í•˜ê¸°", am_text)

    except Exception as e:
        st.error(f"ì˜¤ì „ ì˜¤ë¥˜: {e}")

# =====================================
# 4ï¸âƒ£ ì˜¤í›„ ë°°ì •
# =====================================
st.markdown("---")
st.markdown("## 4ï¸âƒ£ ì˜¤í›„ ê·¼ë¬´ ë°°ì •")
save_check = st.checkbox("ì´ ê²°ê³¼ë¥¼ ì „ì¼ê·¼ë¬´.jsonìœ¼ë¡œ ì €ì¥", value=True)

if st.button("ğŸ“‹ ì˜¤í›„ ë°°ì • ìƒì„±"):
    try:
        veh1_map = parse_vehicle_map(veh1_lines)
        veh2_map = parse_vehicle_map(veh2_lines)

        # êµì • + ê´„í˜¸ ì œê±°
        cleaned_a, _ = split_and_clean_name_list(a_list, employee_list)

        excl_norms = {normalize_name(x) for x in excluded}
        a_norms = {normalize_name(x) for x in cleaned_a} - excl_norms

        # ğŸ§‘â€ğŸ« ì˜¤í›„ êµì–‘ 3Â·4Â·5
        gy_start = st.session_state.get("am_gy_base_for_pm", prev_gy5)
        used = set()
        gy3 = pick_next_from_cycle(gyoyang_ord, gy_start, a_norms)
        if gy3: used.add(normalize_name(gy3))
        gy4 = pick_next_from_cycle(gyoyang_ord, gy3 or gy_start, a_norms - used)
        if gy4: used.add(normalize_name(gy4))
        gy5 = pick_next_from_cycle(gyoyang_ord, gy4 or gy3 or gy_start, a_norms - used)

        # ğŸ”§ ì˜¤í›„ 1ì¢… ìˆ˜ë™ (êµì–‘ê³¼ ì¤‘ë³µ í—ˆìš©)
        sud_base = st.session_state.get("am_sud_base_for_pm", prev_sd)
        sud_a, last_pick = [], sud_base
        for _ in range(sudong_count):
            cand = pick_next_from_cycle(sudong_ord, last_pick, a_norms)
            if not cand: break
            sud_a.append(cand)
            last_pick = cand

        # ğŸš— ì˜¤í›„ 2ì¢… ìë™ = ì „ì²´ - 1ì¢…
        sud_norms_a = {normalize_name(x) for x in sud_a}
        auto_a = [nm for nm in cleaned_a if normalize_name(nm) in (a_norms - sud_norms_a)]

        # === ì¶œë ¥
        today_key = st.session_state.get("today_key", prev_key)
        lines = []
        if today_key: lines.append(f"ì—´ì‡ : {today_key}")
        if gy3: lines.append(f"3êµì‹œ(êµì–‘): {gy3}")
        if gy4: lines.append(f"4êµì‹œ(êµì–‘): {gy4}")
        if gy5: lines.append(f"5êµì‹œ(êµì–‘): {gy5}")

        if sud_a:
            for nm in sud_a:
                lines.append(f"1ì¢…ìˆ˜ë™: {nm} {mark_car(get_vehicle(nm, veh1_map), repair_cars)}")
            if sudong_count == 2 and len(sud_a) < 2:
                lines.append("â€» ìˆ˜ë™ ê°€ëŠ¥ ì¸ì›ì´ 1ëª…ì…ë‹ˆë‹¤.")
        else:
            lines.append("1ì¢…ìˆ˜ë™: (ë°°ì •ì ì—†ìŒ)")

        if auto_a:
            lines.append("2ì¢… ìë™:")
            for nm in auto_a:
                lines.append(f" â€¢ {nm} {mark_car(get_vehicle(nm, veh2_map), repair_cars)}")
        else:
            lines.append("2ì¢… ìë™: (ë°°ì •ì ì—†ìŒ)")

        # === ì˜¤ì „ ëŒ€ë¹„ ë¹„êµ
        lines.append("")
        lines.append("ì˜¤ì „ ëŒ€ë¹„ ë¹„êµ:")
        am_drivers = st.session_state.get("am_driver_names", [])
        am_norms = {normalize_name(x) for x in am_drivers}
        pm_drivers = sud_a + auto_a
        pm_norms = {normalize_name(x) for x in pm_drivers}

        new_joiners = sorted([nm for nm in pm_drivers if normalize_name(nm) not in am_norms])
        missing = sorted([nm for nm in am_drivers if normalize_name(nm) not in pm_norms])

        if new_joiners: lines.append(" â€¢ ì‹ ê·œ íˆ¬ì…: " + ", ".join(new_joiners))
        if missing:     lines.append(" â€¢ ë¹ ì§„ ì¸ì›: " + ", ".join(missing))
        if not new_joiners and not missing:
            lines.append(" â€¢ ë³€ë™ ì—†ìŒ")

        # === ë¯¸ë°°ì • ì°¨ëŸ‰ (ì˜¤ì „ì— ìˆì—ˆëŠ”ë° ì˜¤í›„ì— ì—†ëŠ” ì°¨ëŸ‰ë§Œ)
        am_c1 = set(st.session_state.get("am_cars_1", []))
        am_c2 = set(st.session_state.get("am_cars_2", []))
        pm_c1 = {get_vehicle(x, veh1_map) for x in sud_a if get_vehicle(x, veh1_map)}
        pm_c2 = {get_vehicle(x, veh2_map) for x in auto_a if get_vehicle(x, veh2_map)}
        un1 = sorted([c for c in am_c1 if c and c not in pm_c1])
        un2 = sorted([c for c in am_c2 if c and c not in pm_c2])
        if un1 or un2:
            lines.append("")
            lines.append("ë¯¸ë°°ì • ì°¨ëŸ‰:")
            if un1:
                lines.append(" [1ì¢… ìˆ˜ë™]")
                for c in un1: lines.append(f"  â€¢ {c} ë§ˆê°")
            if un2:
                lines.append(" [2ì¢… ìë™]")
                for c in un2: lines.append(f"  â€¢ {c} ë§ˆê°")

        pm_text = "\n".join(lines)
        st.markdown("### ğŸ“‹ ì˜¤í›„ ê²°ê³¼")
        st.code(pm_text, language="text")
        clipboard_copy_button("ğŸ“‹ ê²°ê³¼ ë³µì‚¬í•˜ê¸°", pm_text)

        # âœ… ì „ì¼ ì €ì¥
        if save_check:
            data = {
                "ì—´ì‡ ": today_key,
                "êµì–‘_5êµì‹œ": gy5 or gy4 or gy3 or prev_gy5,
                "1ì¢…ìˆ˜ë™": (sud_a[-1] if sud_a else prev_sd)
            }
            save_json(PREV_FILE, data)
            st.success("ì „ì¼ê·¼ë¬´.json ì—…ë°ì´íŠ¸ ì™„ë£Œ âœ…")

    except Exception as e:
        st.error(f"ì˜¤í›„ ì˜¤ë¥˜: {e}")

