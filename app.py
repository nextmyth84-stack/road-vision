# app.py â€” ë„ë¡œì£¼í–‰ ê·¼ë¬´ìë™ë°°ì • v7.16 (ì™„ì „ë³¸)
import streamlit as st
from openai import OpenAI
import base64, re, json, os, difflib
from difflib import SequenceMatcher, get_close_matches

# =====================================
# í˜ì´ì§€ ì„¤ì •
# =====================================
st.set_page_config(page_title="ë„ë¡œì£¼í–‰ ê·¼ë¬´ìë™ë°°ì •", layout="wide")
st.markdown("<h3 style='text-align:center; font-size:22px;'>ğŸš— ë„ë¡œì£¼í–‰ ê·¼ë¬´ìë™ë°°ì •</h3>", unsafe_allow_html=True)

# =====================================
# OpenAI ì´ˆê¸°í™”
# =====================================
try:
    client = OpenAI(api_key=st.secrets["general"]["OPENAI_API_KEY"])
except Exception:
    st.error("âš ï¸ OPENAI_API_KEY ì„¤ì • í•„ìš”")
    st.stop()
MODEL_NAME = "gpt-4o"

# =====================================
# ê²½ë¡œ ì„¤ì •
# =====================================
PREV_FILE = "ì „ì¼ê·¼ë¬´.json"
EMP_FILE = "ê·¼ë¬´ìëª…ë‹¨.json"
COURSE_FILE = "ì½”ìŠ¤ì ê²€ê²°ê³¼.json"

# =====================================
# JSON ìœ í‹¸
# =====================================
def load_json(path, default):
    if os.path.exists(path):
        try:
            with open(path, "r", encoding="utf-8") as f:
                return json.load(f)
        except:
            return default
    return default

def save_json(path, data):
    with open(path, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)

# =====================================
# ì´ë¦„ ì •ê·œí™” ë° êµì •
# =====================================
def normalize_name(s): 
    return re.sub(r"[^ê°€-í£]", "", re.sub(r"\(.*?\)", "", s or ""))

def _sim(a, b): 
    return SequenceMatcher(None, a, b).ratio()

def _apply_ocr_fixups(s):
    fixes = {"êµ°":"ê· ","ìš©":"ì˜","ë‹ˆ":"ë¯¸","ì¡":"ì •","ì„²":"ì„"}
    for k, v in fixes.items():
        s = s.replace(k, v)
    return s

def correct_name(name, valid_names, cutoff=0.55):
    """OCR êµì • â€” ê·¼ë¬´ìëª…ë‹¨ ê¸°ë°˜ ê·¼ì‚¬ êµì •"""
    if not valid_names:
        return name

    norm_map = {normalize_name(v): v for v in valid_names}
    n0 = normalize_name(name)
    if n0 in norm_map:
        return norm_map[n0]

    # 1ì°¨ ê·¼ì‚¬ ë§¤ì¹­
    m = get_close_matches(n0, list(norm_map.keys()), n=1, cutoff=cutoff)
    if m:
        return norm_map[m[0]]

    # 2ì°¨: ê²½ë¯¸í•œ OCR ë³´ì • í›„ ì¬ì‹œë„
    n1 = normalize_name(_apply_ocr_fixups(name))
    if n1 in norm_map:
        return norm_map[n1]
    m2 = get_close_matches(n1, list(norm_map.keys()), n=1, cutoff=cutoff - 0.02)
    if m2:
        return norm_map[m2[0]]

    # 3ì°¨: ì§ì ‘ ìœ ì‚¬ë„ ë¹„êµ
    best, score = name, 0.0
    for c in norm_map:
        sc = _sim(n1, c)
        if sc > score:
            best, score = c, sc
    if score >= cutoff - 0.05:
        return norm_map[best]

    return name

# =====================================
# ë³µì‚¬ ë²„íŠ¼ (JS ì•ˆì „ ë Œë”ë§)
# =====================================
def clipboard_copy_button(label, text):
    """ì½”ë“œë¡œ ë³´ì´ì§€ ì•Šê²Œ ì•ˆì „í•œ í´ë¦½ë³´ë“œ ë³µì‚¬ ë²„íŠ¼"""
    safe_text = text.replace("\\", "\\\\").replace("`", "\\`").replace("\n", "\\n")
    js = f"""
    <button onclick="navigator.clipboard.writeText(`{safe_text}`); alert('ë³µì‚¬ë˜ì—ˆìŠµë‹ˆë‹¤ âœ…');"
        style="background-color:#4CAF50;color:white;border:none;border-radius:6px;
        padding:6px 14px;cursor:pointer;font-size:14px;">
        {label}
    </button>
    """
    st.markdown(js, unsafe_allow_html=True)

# =====================================
# 2/3 â€” ìˆœë²ˆÂ·ì°¨ëŸ‰í‘œ íŒŒì¼/ì‚¬ì´ë“œë°” + OCR + ì˜¤ì „ ë°°ì •
# =====================================

# â–¼ ìˆœë²ˆ/ì°¨ëŸ‰í‘œ íŒŒì¼ê²½ë¡œ
KEY_FILE   = "ì—´ì‡ ìˆœë²ˆ.json"
GY_FILE    = "êµì–‘ìˆœë²ˆ.json"
SUD_FILE   = "1ì¢…ìˆ˜ë™ìˆœë²ˆ.json"
VEH1_FILE  = "1ì¢…ìˆ˜ë™ì°¨ëŸ‰.json"
VEH2_FILE  = "2ì¢…ìë™ì°¨ëŸ‰.json"

# â–¼ ê¸°ë³¸ê°’
default_key = ["ê¶Œí•œì†”","ê¹€ë‚¨ê· ","ê¹€ë©´ì •","ê¹€ì„±ì—°","ê¹€ì§€ì€","ì•ˆìœ ë¯¸","ìœ¤ì—¬í—Œ","ìœ¤ì›ì‹¤","ì´ë‚˜ë˜","ì´í˜¸ì„","ì¡°ìœ¤ì˜","ì¡°ì •ë˜"]
default_gy  = ["ê¶Œí•œì†”","ê¹€ë‚¨ê· ","ê¹€ë©´ì •","ê¹€ë³‘ìš±","ê¹€ì„±ì—°","ê¹€ì£¼í˜„","ê¹€ì§€ì€","ì•ˆìœ ë¯¸","ì´í˜¸ì„","ì¡°ì •ë˜"]
default_sd  = ["ê¶Œí•œì†”","ê¹€ë‚¨ê· ","ê¹€ì„±ì—°","ê¹€ì£¼í˜„","ì´í˜¸ì„","ì¡°ì •ë˜"]
default_veh1 = ["2í˜¸ ì¡°ì •ë˜","5í˜¸ ê¶Œí•œì†”","7í˜¸ ê¹€ë‚¨ê· ","8í˜¸ ì´í˜¸ì„","9í˜¸ ê¹€ì£¼í˜„","10í˜¸ ê¹€ì„±ì—°"]
default_veh2 = ["4í˜¸ ê¹€ë‚¨ê· ","5í˜¸ ê¹€ë³‘ìš±","6í˜¸ ê¹€ì§€ì€","12í˜¸ ì•ˆìœ ë¯¸","14í˜¸ ê¹€ë©´ì •","15í˜¸ ì´í˜¸ì„",
                "17í˜¸ ê¹€ì„±ì—°","18í˜¸ ê¶Œí•œì†”","19í˜¸ ê¹€ì£¼í˜„","22í˜¸ ì¡°ì •ë˜"]

# â–¼ íŒŒì¼ ì—†ìœ¼ë©´ ìƒì„±
if not os.path.exists(KEY_FILE):  save_json(KEY_FILE, default_key)
if not os.path.exists(GY_FILE):   save_json(GY_FILE,  default_gy)
if not os.path.exists(SUD_FILE):  save_json(SUD_FILE, default_sd)
if not os.path.exists(VEH1_FILE): save_json(VEH1_FILE, default_veh1)
if not os.path.exists(VEH2_FILE): save_json(VEH2_FILE, default_veh2)
if not os.path.exists(PREV_FILE): save_json(PREV_FILE, {"ì—´ì‡ ":"", "êµì–‘_5êµì‹œ":"", "1ì¢…ìˆ˜ë™":""})
if not os.path.exists(COURSE_FILE): save_json(COURSE_FILE, [])

# â–¼ í˜„ì¬ ê°’ ë¡œë“œ
key_order     = load_json(KEY_FILE, default_key)
gyoyang_order = load_json(GY_FILE,  default_gy)
sudong_order  = load_json(SUD_FILE, default_sd)
veh1_lines    = load_json(VEH1_FILE, default_veh1)
veh2_lines    = load_json(VEH2_FILE, default_veh2)
employees     = load_json(EMP_FILE, employees if 'employees' in globals() else [])
_prev         = load_json(PREV_FILE, {"ì—´ì‡ ":"", "êµì–‘_5êµì‹œ":"", "1ì¢…ìˆ˜ë™":""})
prev_key, prev_gyoyang5, prev_sudong = _prev.get("ì—´ì‡ ",""), _prev.get("êµì–‘_5êµì‹œ",""), _prev.get("1ì¢…ìˆ˜ë™","")

st.info(f"ì „ì¼ ë¶ˆëŸ¬ì˜´ â†’ ì—´ì‡ :{prev_key or '-'}, êµì–‘5:{prev_gyoyang5 or '-'}, 1ì¢…:{prev_sudong or '-'}")

# ---------- ì‚¬ì´ë“œë°”(ìˆ¨ê¹€ í¸ì§‘) ----------
st.sidebar.header("ğŸ“‹ ìˆœë²ˆí‘œ / ì°¨ëŸ‰í‘œ / ì „ì¼ê°’ / ê·¼ë¬´ìëª…ë‹¨")

def _list(s): return [x.strip() for x in s.splitlines() if x.strip()]

with st.sidebar.expander("ğŸ”‘ ì—´ì‡  ìˆœë²ˆ", expanded=False):
    key_text = st.text_area("ì—´ì‡  ìˆœë²ˆ", "\n".join(key_order), height=180, key="key_text")
    if st.button("ğŸ’¾ ì—´ì‡ ìˆœë²ˆ ì €ì¥"):
        save_json(KEY_FILE, _list(key_text)); st.sidebar.success("ì €ì¥ ì™„ë£Œ"); st.rerun()

with st.sidebar.expander("ğŸ“˜ êµì–‘ ìˆœë²ˆ", expanded=False):
    gy_text = st.text_area("êµì–‘ ìˆœë²ˆ", "\n".join(gyoyang_order), height=180, key="gy_text")
    if st.button("ğŸ’¾ êµì–‘ìˆœë²ˆ ì €ì¥"):
        save_json(GY_FILE, _list(gy_text)); st.sidebar.success("ì €ì¥ ì™„ë£Œ"); st.rerun()

with st.sidebar.expander("ğŸ”§ 1ì¢… ìˆ˜ë™ ìˆœë²ˆ", expanded=False):
    sd_text = st.text_area("1ì¢… ìˆ˜ë™ ìˆœë²ˆ", "\n".join(sudong_order), height=150, key="sd_text")
    st.session_state["sudong_count"] = st.radio("1ì¢… ìˆ˜ë™ ì¸ì›ìˆ˜", [1, 2], index=0, horizontal=True, key="sudong_count_radio")
    if st.button("ğŸ’¾ 1ì¢… ìˆœë²ˆ ì €ì¥"):
        save_json(SUD_FILE, _list(sd_text)); st.sidebar.success("ì €ì¥ ì™„ë£Œ"); st.rerun()

with st.sidebar.expander("ğŸš— ì°¨ëŸ‰í‘œ (1ì¢… ìˆ˜ë™)", expanded=False):
    v1_text = st.text_area("1ì¢… ìˆ˜ë™ ì°¨ëŸ‰í‘œ", "\n".join(veh1_lines), height=150, key="v1_text")
    if st.button("ğŸ’¾ 1ì¢… ì°¨ëŸ‰í‘œ ì €ì¥"):
        save_json(VEH1_FILE, _list(v1_text)); st.sidebar.success("ì €ì¥ ì™„ë£Œ"); st.rerun()

with st.sidebar.expander("ğŸš˜ ì°¨ëŸ‰í‘œ (2ì¢… ìë™)", expanded=False):
    v2_text = st.text_area("2ì¢… ìë™ ì°¨ëŸ‰í‘œ", "\n".join(veh2_lines), height=180, key="v2_text")
    if st.button("ğŸ’¾ 2ì¢… ì°¨ëŸ‰í‘œ ì €ì¥"):
        save_json(VEH2_FILE, _list(v2_text)); st.sidebar.success("ì €ì¥ ì™„ë£Œ"); st.rerun()

with st.sidebar.expander("ğŸ—“ ì „ì¼ ê°’ í™•ì¸/ìˆ˜ì •", expanded=False):
    p_key  = st.text_input("ì „ì¼ ì—´ì‡ ", value=prev_key)
    p_gy5  = st.text_input("ì „ì¼ êµì–‘5", value=prev_gyoyang5)
    p_sd   = st.text_input("ì „ì¼ 1ì¢…ìˆ˜ë™", value=prev_sudong)
    if st.button("ğŸ’¾ ì „ì¼ê°’ ì €ì¥"):
        save_json(PREV_FILE, {"ì—´ì‡ ": p_key, "êµì–‘_5êµì‹œ": p_gy5, "1ì¢…ìˆ˜ë™": p_sd})
        st.sidebar.success("ì €ì¥ ì™„ë£Œ")

with st.sidebar.expander("ğŸ‘¥ ì „ì²´ ê·¼ë¬´ìëª…ë‹¨ (OCR êµì •ìš©)", expanded=False):
    emp_text = st.text_area("ê·¼ë¬´ìëª…ë‹¨ (í•œ ì¤„ë‹¹ í•œ ëª…)", "\n".join(employees), height=200, key="emp_text")
    if st.button("ğŸ’¾ ê·¼ë¬´ìëª…ë‹¨ ì €ì¥"):
        new_emps = _list(emp_text)
        save_json(EMP_FILE, new_emps); st.sidebar.success("ì €ì¥ ì™„ë£Œ"); st.rerun()

# ---------- ì°¨ëŸ‰í‘œ íŒŒì‹± ----------
def parse_vehicle_map_from_lines(lines):
    text = "\n".join(lines)
    m = {}
    for line in text.splitlines():
        p = line.strip().split()
        if len(p) >= 2:
            car = p[0]
            name = " ".join(p[1:])
            m[name] = car
    return m

veh1 = parse_vehicle_map_from_lines(veh1_lines)
veh2 = parse_vehicle_map_from_lines(veh2_lines)

def get_vehicle(name, veh_map):
    nkey = normalize_name(name)
    for k, v in veh_map.items():
        if normalize_name(k) == nkey:
            return v
    return ""

def pick_next_from_cycle(cycle, last, allowed_norms: set):
    if not cycle: return None
    cyl_norm = [normalize_name(x) for x in cycle]
    last_norm = normalize_name(last)
    start = (cyl_norm.index(last_norm) + 1) % len(cycle) if last_norm in cyl_norm else 0
    for i in range(len(cycle) * 2):
        cand = cycle[(start + i) % len(cycle)]
        if normalize_name(cand) in allowed_norms:
            return cand
    return None

# ---------- GPT OCR (êµì • + ê·¼ë¬´ì œì™¸ + ì½”ìŠ¤ì ê²€ A/B í•©ë¶ˆ) ----------
def gpt_extract(img_bytes, detect_excluded=True):
    b64 = base64.b64encode(img_bytes).decode()
    user = (
        "ì´ ì´ë¯¸ì§€ëŠ” ìš´ì „ë©´í—ˆì‹œí—˜ ê·¼ë¬´í‘œì…ë‹ˆë‹¤.\n"
        "1) ë„ë¡œì£¼í–‰ ê·¼ë¬´ì ì´ë¦„ì„ JSONìœ¼ë¡œ ì¶”ì¶œí•˜ì„¸ìš”.\n"
        "2) ì´ë¦„ ì˜† ê´„í˜¸(A-í•©/B-ë¶ˆ ë“±)ëŠ” ê·¸ëŒ€ë¡œ í¬í•¨í•´ ë°˜í™˜í•˜ì„¸ìš”.\n"
        "3) 'ì§€ì›','ì¸í„´','ì—°ìˆ˜' í¬í•¨ìëŠ” ì œì™¸í•˜ì„¸ìš”.\n"
    )
    if detect_excluded:
        user += "4) ìƒë‹¨ì˜ 'íœ´ê°€','ì¶œì¥','êµìœ¡','ê³µê°€','ì—°ê°€','ì—°ì°¨','ëŒë´„' ì¤„ì˜ ì´ë¦„ì„ 'excluded' ë°°ì—´ë¡œ ë°˜í™˜í•˜ì„¸ìš”.\n"
    user += 'ë°˜í™˜ ì˜ˆì‹œ: {"names": ["ê¹€ì§€ì€(A-í•©)","ì¡°ìœ¤ì˜(B ë¶ˆ)","ì´í˜¸ì„"], "excluded": ["ì•ˆìœ ë¯¸"]}'

    try:
        res = client.chat.completions.create(
            model=MODEL_NAME,
            messages=[
                {"role": "system", "content": "í‘œì—ì„œ ê·¼ë¬´ì/ì½”ìŠ¤/ì œì™¸ìë¥¼ JSONìœ¼ë¡œ ì¶”ì¶œ"},
                {"role": "user", "content": [
                    {"type": "text", "text": user},
                    {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{b64}"}}
                ]}
            ],
        )
        raw = (res.choices[0].message.content or "").strip()
        js = json.loads(re.search(r"\{.*\}", raw, re.S).group(0))

        full = [n.strip() for n in js.get("names", []) if not re.search(r"(ì§€ì›|ì¸í„´|ì—°ìˆ˜)", n)]
        names, course_info = [], []

        for n in full:
            # ê´„í˜¸ì—ì„œ A/B + í•©/ë¶ˆ íŒì • (íŠ¹ìˆ˜ë¬¸ìÂ·ê³µë°± ì œê±° í›„ A/Bë§Œ ê¸°ì¤€)
            m = re.search(r"\(([A-Za-zê°€-í£\- ]+)\)", n)
            pure = re.sub(r"\(.*?\)", "", n).strip()   # ìˆœë²ˆìš© ì´ë¦„
            corrected = correct_name(pure, employees)  # ì •ì‹ í‘œê¸°ë¡œ êµì •

            if m:
                text = m.group(1).replace("-", "").replace(" ", "").upper()
                course_type  = "A" if "A" in text else "B" if "B" in text else None
                result_type  = "í•©ê²©" if "í•©" in text else "ë¶ˆí•©ê²©" if "ë¶ˆ" in text else None
                if course_type and result_type:
                    course_info.append({"name": corrected, "course": f"{course_type}ì½”ìŠ¤ {result_type}"})

            names.append(corrected)

        excluded = js.get("excluded", []) if detect_excluded else []
        excluded = [correct_name(x, employees) for x in excluded]

        save_json(COURSE_FILE, course_info)  # ì˜¤ì „ ì¶œë ¥ìš© ì €ì¥
        return names, course_info, excluded

    except Exception as e:
        st.error(f"OCR ì‹¤íŒ¨: {e}")
        return [], [], []

# ---------- ì˜¤ì „ ì´ë¯¸ì§€/OCR ----------
st.markdown("<h4>1ï¸âƒ£ ì˜¤ì „ ê·¼ë¬´í‘œ ì—…ë¡œë“œ</h4>", unsafe_allow_html=True)
m_file = st.file_uploader("ğŸ“¸ ì˜¤ì „ ê·¼ë¬´í‘œ", type=["png","jpg","jpeg"], key="m_file")

if st.button("ğŸ§  ì˜¤ì „ GPT ì¸ì‹"):
    if not m_file:
        st.warning("ì˜¤ì „ ì´ë¯¸ì§€ë¥¼ ì—…ë¡œë“œí•˜ì„¸ìš”.")
    else:
        with st.spinner("ì˜¤ì „ ê·¼ë¬´í‘œ ë¶„ì„ ì¤‘..."):
            m_names, _course, excl = gpt_extract(m_file.read(), detect_excluded=True)
            st.session_state.m_names_raw = m_names
            st.session_state.excluded_auto = excl
            st.success(f"ì˜¤ì „ ì¸ì‹ ì™„ë£Œ: ê·¼ë¬´ì {len(m_names)}ëª…, ê·¼ë¬´ì œì™¸ {len(excl)}ëª… (ì½”ìŠ¤ ê²°ê³¼ ì €ì¥)")
        st.rerun()

# ---------- ê·¼ë¬´ì œì™¸ì/ì˜¤ì „ëª…ë‹¨ í™•ì¸ ----------
st.markdown("### ğŸš« ê·¼ë¬´ì œì™¸ì (ìë™ì¶”ì¶œ í›„ ìˆ˜ì • ê°€ëŠ¥)")
excluded_text = st.text_area("ìë™ ì¸ì‹ëœ ê·¼ë¬´ì œì™¸ì", "\n".join(st.session_state.get("excluded_auto", [])), height=90)
excluded = {x.strip() for x in excluded_text.splitlines() if x.strip()}

morning_text = st.text_area("ì˜¤ì „ ê·¼ë¬´ì (í•„ìš” ì‹œ ìˆ˜ì •)", "\n".join(st.session_state.get("m_names_raw", [])), height=150)
m_list = [x.strip() for x in morning_text.splitlines() if x.strip()]
m_allowed = {normalize_name(x) for x in m_list} - {normalize_name(x) for x in excluded}

# ---------- ì˜¤ì „ ë°°ì • ----------
def morning_assign():
    lines = []
    sudong_count = st.session_state.get("sudong_count_radio", 1)

    # ğŸ”‘ ì—´ì‡ 
    today_key = pick_next_from_cycle(key_order, prev_key, m_allowed) if key_order else ""
    st.session_state.today_key = today_key

    # ğŸ§‘â€ğŸ« êµì–‘ 1Â·2
    gy1 = pick_next_from_cycle(gyoyang_order, prev_gyoyang5, m_allowed) if gyoyang_order else None
    gy1_norm = normalize_name(gy1) if gy1 else None
    gy2 = pick_next_from_cycle(gyoyang_order, gy1 or prev_gyoyang5, m_allowed - ({gy1_norm} if gy1_norm else set())) if gyoyang_order else None
    st.session_state.gyoyang_base_for_pm = gy2 or gy1 or prev_gyoyang5

    # ğŸ”§ 1ì¢… ìˆ˜ë™
    sud_m, last = [], prev_sudong
    for _ in range(sudong_count):
        pick = pick_next_from_cycle(sudong_order, last, m_allowed - {normalize_name(x) for x in sud_m})
        if not pick: break
        sud_m.append(pick); last = pick
    st.session_state.sudong_base_for_pm = sud_m[-1] if sud_m else prev_sudong

    # ğŸš— 2ì¢… ìë™
    sud_norms_m = {normalize_name(x) for x in sud_m}
    auto_m = [x for x in m_list if normalize_name(x) in (m_allowed - sud_norms_m)]

    # ì˜¤í›„ ëŒ€ë¹„/ë¯¸ë°°ì • ì°¨ëŸ‰ ê³„ì‚°ìš© ì €ì¥
    st.session_state.morning_auto_names = auto_m + sud_m   # ì˜¤ì „ ìš´ì „ ì „ì²´(1+2)
    st.session_state.morning_cars_1 = [get_vehicle(x, veh1) for x in sud_m if get_vehicle(x, veh1)]
    st.session_state.morning_cars_2 = [get_vehicle(x, veh2) for x in auto_m if get_vehicle(x, veh2)]

    # === ì¶œë ¥(ì˜¤ì „)
    if today_key: lines.append(f"ì—´ì‡ : {today_key}")
    if gy1: lines.append(f"1êµì‹œ(êµì–‘): {gy1}")
    if gy2: lines.append(f"2êµì‹œ(êµì–‘): {gy2}")

    if sud_m:
        for nm in sud_m:
            lines.append(f"1ì¢…ìˆ˜ë™: {nm} {get_vehicle(nm, veh1) or ''}")
        if sudong_count == 2 and len(sud_m) < 2:
            lines.append("â€» ìˆ˜ë™ ê°€ëŠ¥ ì¸ì›ì´ 1ëª…ì…ë‹ˆë‹¤.")
    else:
        lines.append("1ì¢…ìˆ˜ë™: (ë°°ì •ì ì—†ìŒ)")

    if auto_m:
        lines.append("2ì¢… ìë™:")
        for nm in auto_m:
            lines.append(f" â€¢ {nm} {get_vehicle(nm, veh2) or ''}")
    else:
        lines.append("2ì¢… ìë™: (ë°°ì •ì ì—†ìŒ)")

    # âœ… ì½”ìŠ¤ì ê²€ (ì˜¤ì „ë§Œ ì¶œë ¥)
    course_info = load_json(COURSE_FILE, [])
    if course_info:
        lines.append("")
        lines.append("ì½”ìŠ¤ì ê²€ ê²°ê³¼:")
        a_pass = [x["name"] for x in course_info if "Aì½”ìŠ¤ í•©ê²©" in x.get("course","")]
        a_fail = [x["name"] for x in course_info if "Aì½”ìŠ¤ ë¶ˆí•©ê²©" in x.get("course","")]
        b_pass = [x["name"] for x in course_info if "Bì½”ìŠ¤ í•©ê²©" in x.get("course","")]
        b_fail = [x["name"] for x in course_info if "Bì½”ìŠ¤ ë¶ˆí•©ê²©" in x.get("course","")]
        if a_pass: lines.append(" â€¢ Aì½”ìŠ¤ í•©ê²©: " + ", ".join(a_pass))
        if a_fail: lines.append(" â€¢ Aì½”ìŠ¤ ë¶ˆí•©ê²©: " + ", ".join(a_fail))
        if b_pass: lines.append(" â€¢ Bì½”ìŠ¤ í•©ê²©: " + ", ".join(b_pass))
        if b_fail: lines.append(" â€¢ Bì½”ìŠ¤ ë¶ˆí•©ê²©: " + ", ".join(b_fail))

    result_text = "\n".join(lines)
    st.markdown("### ğŸ“‹ ì˜¤ì „ ê²°ê³¼")
    st.code(result_text, language="text")
    clipboard_copy_button("ğŸ“‹ ê²°ê³¼ ë³µì‚¬í•˜ê¸°", result_text)

# ë²„íŠ¼
if st.button("ğŸ“‹ ì˜¤ì „ ë°°ì • ìƒì„±"):
    try:
        morning_assign()
    except Exception as e:
        st.error(f"ì˜¤ì „ ì˜¤ë¥˜: {e}")

# =====================================
# 3/3 â€” ì˜¤í›„ OCR/ë°°ì • + ì‹ ê·œíˆ¬ì…/ë¹ ì§„ì¸ì› + ë¯¸ë°°ì •ì°¨ëŸ‰ + ì „ì¼ì €ì¥
# =====================================

st.markdown("<h4>2ï¸âƒ£ ì˜¤í›„ ê·¼ë¬´í‘œ ì—…ë¡œë“œ</h4>", unsafe_allow_html=True)
a_file = st.file_uploader("ğŸ“¸ ì˜¤í›„ ê·¼ë¬´í‘œ", type=["png","jpg","jpeg"], key="a_file")

if st.button("ğŸ§  ì˜¤í›„ GPT ì¸ì‹"):
    if not a_file:
        st.warning("ì˜¤í›„ ì´ë¯¸ì§€ë¥¼ ì—…ë¡œë“œí•˜ì„¸ìš”.")
    else:
        with st.spinner("ì˜¤í›„ ê·¼ë¬´í‘œ ë¶„ì„ ì¤‘..."):
            a_names, _course, _ = gpt_extract(a_file.read(), detect_excluded=False)
            st.session_state.a_names_raw = a_names
            st.success(f"ì˜¤í›„ ì¸ì‹ ì™„ë£Œ: ê·¼ë¬´ì {len(a_names)}ëª…")
        st.rerun()

afternoon_text = st.text_area("ì˜¤í›„ ê·¼ë¬´ì (í•„ìš” ì‹œ ìˆ˜ì •)", "\n".join(st.session_state.get("a_names_raw", [])), height=150)
a_list = [x.strip() for x in afternoon_text.splitlines() if x.strip()]

# ì œì™¸ì ì„¸ì´í”„ê°€ë“œ (ì˜¤ì „ OCR ìƒëµ ì‹œ ëŒ€ë¹„)
excluded_norms = {normalize_name(x) for x in st.session_state.get("excluded_auto", [])}

save_check = st.checkbox("ì´ ê²°ê³¼ë¥¼ ì „ì¼ê·¼ë¬´.jsonìœ¼ë¡œ ì €ì¥", value=True)

def afternoon_assign():
    lines = []
    sudong_count = st.session_state.get("sudong_count_radio", 1)

    today_key = st.session_state.get("today_key", prev_key)
    gy_start  = st.session_state.get("gyoyang_base_for_pm", prev_gyoyang5)
    sud_base  = st.session_state.get("sudong_base_for_pm", prev_sudong)

    a_norms = {normalize_name(x) for x in a_list} - excluded_norms

    # ğŸ§‘â€ğŸ« êµì–‘ 3Â·4Â·5
    used = set()
    gy3 = pick_next_from_cycle(gyoyang_order, gy_start, a_norms)
    if gy3: used.add(normalize_name(gy3))
    gy4 = pick_next_from_cycle(gyoyang_order, gy3 or gy_start, a_norms - used)
    if gy4: used.add(normalize_name(gy4))
    gy5 = pick_next_from_cycle(gyoyang_order, gy4 or gy3 or gy_start, a_norms - used)

    # ğŸ”§ 1ì¢… ìˆ˜ë™ (êµì–‘ê³¼ ì¤‘ë³µ í—ˆìš©)
    sud_a, last = [], sud_base
    for _ in range(sudong_count):
        pick = pick_next_from_cycle(sudong_order, last, a_norms)
        if not pick: break
        sud_a.append(pick); last = pick

    # ğŸš— 2ì¢… ìë™ (1ì¢… ì œì™¸)
    sud_norms_a = {normalize_name(x) for x in sud_a}
    auto_a = [x for x in a_list if normalize_name(x) in (a_norms - sud_norms_a)]

    # === ì¶œë ¥
    if today_key: lines.append(f"ì—´ì‡ : {today_key}")
    if gy3: lines.append(f"3êµì‹œ(êµì–‘): {gy3}")
    if gy4: lines.append(f"4êµì‹œ(êµì–‘): {gy4}")
    if gy5: lines.append(f"5êµì‹œ(êµì–‘): {gy5}")

    if sud_a:
        for nm in sud_a:
            lines.append(f"1ì¢…ìˆ˜ë™: {nm} {get_vehicle(nm, veh1) or ''}")
        if sudong_count == 2 and len(sud_a) < 2:
            lines.append("â€» ìˆ˜ë™ ê°€ëŠ¥ ì¸ì›ì´ 1ëª…ì…ë‹ˆë‹¤.")
    else:
        lines.append("1ì¢…ìˆ˜ë™: (ë°°ì •ì ì—†ìŒ)")

    if auto_a:
        lines.append("2ì¢… ìë™:")
        for nm in auto_a:
            lines.append(f" â€¢ {nm} {get_vehicle(nm, veh2) or ''}")
    else:
        lines.append("2ì¢… ìë™: (ë°°ì •ì ì—†ìŒ)")

    # === ì˜¤ì „ ëŒ€ë¹„ ë¹„êµ (ì˜¤ì „ ë¹„ê·¼ë¬´ â†’ ì˜¤í›„ ê·¼ë¬´ ì‹ ê·œ íˆ¬ì… / ì˜¤ì „ ê·¼ë¬´ â†’ ì˜¤í›„ ë¹„ê·¼ë¬´ ë¹ ì§)
    lines.append("")
    lines.append("ì˜¤ì „ ëŒ€ë¹„ ë¹„êµ:")

    morning_drivers   = st.session_state.get("morning_auto_names", [])  # ì˜¤ì „ ìš´ì „ ì „ì²´(1+2)
    morning_norms     = {normalize_name(x) for x in morning_drivers}
    afternoon_drivers = auto_a + sud_a
    afternoon_norms   = {normalize_name(x) for x in afternoon_drivers}

    new_joiners = sorted([nm for nm in afternoon_drivers if normalize_name(nm) not in morning_norms])
    missing     = sorted([nm for nm in morning_drivers  if normalize_name(nm) not in afternoon_norms])

    if new_joiners: lines.append(" â€¢ ì‹ ê·œ íˆ¬ì… ì¸ì›: " + ", ".join(new_joiners))
    if missing:     lines.append(" â€¢ ë¹ ì§„ ì¸ì›: " + ", ".join(missing))
    if not new_joiners and not missing:
        lines.append(" â€¢ ë³€ë™ ì—†ìŒ")

    # === ë¯¸ë°°ì • ì°¨ëŸ‰ (ì˜¤ì „ì— ìˆì—ˆëŠ”ë° ì˜¤í›„ì— ì—†ëŠ” ê²ƒë§Œ)
    m_cars_1 = set(st.session_state.get("morning_cars_1", []))
    m_cars_2 = set(st.session_state.get("morning_cars_2", []))
    a_cars_1 = {get_vehicle(x, veh1) for x in sud_a if get_vehicle(x, veh1)}
    a_cars_2 = {get_vehicle(x, veh2) for x in auto_a if get_vehicle(x, veh2)}

    unassigned_1 = sorted([c for c in m_cars_1 if c not in a_cars_1])
    unassigned_2 = sorted([c for c in m_cars_2 if c not in a_cars_2])

    if unassigned_1 or unassigned_2:
        lines.append("")
        lines.append("ë¯¸ë°°ì • ì°¨ëŸ‰:")
        if unassigned_1:
            lines.append(" [1ì¢… ìˆ˜ë™]")
            for c in unassigned_1: lines.append(f"  â€¢ {c} ë§ˆê°")
        if unassigned_2:
            lines.append(" [2ì¢… ìë™]")
            for c in unassigned_2: lines.append(f"  â€¢ {c} ë§ˆê°")

    # ì¶œë ¥ + ë³µì‚¬
    result_text = "\n".join(lines)
    st.markdown("### ğŸ“‹ ì˜¤í›„ ê²°ê³¼")
    st.code(result_text, language="text")
    clipboard_copy_button("ğŸ“‹ ê²°ê³¼ ë³µì‚¬í•˜ê¸°", result_text)

    # ì „ì¼ ì €ì¥
    if save_check:
        data = {
            "ì—´ì‡ ": today_key,
            "êµì–‘_5êµì‹œ": gy5 or gy4 or gy3 or prev_gyoyang5,
            "1ì¢…ìˆ˜ë™": (sud_a[-1] if sud_a else prev_sudong)
        }
        save_json(PREV_FILE, data)
        st.success("ì „ì¼ê·¼ë¬´.json ì—…ë°ì´íŠ¸ ì™„ë£Œ âœ…")

# ë²„íŠ¼
if st.button("ğŸ“‹ ì˜¤í›„ ë°°ì • ìƒì„±"):
    try:
        afternoon_assign()
    except Exception as e:
        st.error(f"ì˜¤í›„ ì˜¤ë¥˜: {e}")
